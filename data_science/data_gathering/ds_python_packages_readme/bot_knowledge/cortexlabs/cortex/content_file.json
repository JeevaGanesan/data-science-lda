{"results": {"content_files": {"name": "README.md", "path": "README.md", "content": "# Deploy machine learning models in production\n\nCortex is an open source platform for deploying machine learning models as production web services.\n\n<br>\n\n<!-- Delete on release branches -->\n<!-- CORTEX_VERSION_README_MINOR -->\n[install](https://cortex.dev/install) \u2022 [tutorial](https://cortex.dev/iris-classifier) \u2022 [docs](https://cortex.dev) \u2022 [examples](https://github.com/cortexlabs/cortex/tree/0.15/examples) \u2022 [we're hiring](https://angel.co/cortex-labs-inc/jobs) \u2022 [email us](mailto:hello@cortex.dev) \u2022 [chat with us](https://gitter.im/cortexlabs/cortex)<br><br>\n\n<!-- Set header Cache-Control=no-cache on the S3 object metadata (see https://help.github.com/en/articles/about-anonymized-image-urls) -->\n![Demo](https://d1zqebknpdh033.cloudfront.net/demo/gif/v0.13_2.gif)\n\n<br>\n\n## Key features\n\n* **Multi framework:** Cortex supports TensorFlow, PyTorch, scikit-learn, XGBoost, and more.\n* **Autoscaling:** Cortex automatically scales APIs to handle production workloads.\n* **CPU / GPU support:** Cortex can run inference on CPU or GPU infrastructure.\n* **Spot instances:** Cortex supports EC2 spot instances.\n* **Rolling updates:** Cortex updates deployed APIs without any downtime.\n* **Log streaming:** Cortex streams logs from deployed models to your CLI.\n* **Prediction monitoring:** Cortex monitors network metrics and tracks predictions.\n* **Minimal configuration:** Cortex deployments are defined in a single `cortex.yaml` file.\n\n<br>\n\n## Spinning up a cluster\n\nCortex is designed to be self-hosted on any AWS account. You can spin up a cluster with a single command:\n\n<!-- CORTEX_VERSION_README_MINOR -->\n```bash\n# install the CLI on your machine\n$ bash -c \"$(curl -sS https://raw.githubusercontent.com/cortexlabs/cortex/0.15/get-cli.sh)\"\n\n# provision infrastructure on AWS and spin up a cluster\n$ cortex cluster up\n\naws region: us-west-2\naws instance type: g4dn.xlarge\nspot instances: yes\nmin instances: 0\nmax instances: 5\n\naws resource                                cost per hour\n1 eks cluster                               $0.10\n0 - 5 g4dn.xlarge instances for your apis   $0.1578 - $0.526 each (varies based on spot price)\n0 - 5 20gb ebs volumes for your apis        $0.003 each\n1 t3.medium instance for the operator       $0.0416\n1 20gb ebs volume for the operator          $0.003\n2 elastic load balancers                    $0.025 each\n\nyour cluster will cost $0.19 - $2.84 per hour based on the cluster size and spot instance availability\n\n\uffee spinning up your cluster ...\n\nyour cluster is ready!\n```\n\n<br>\n\n## Deploying a model\n\n### Implement your predictor\n\n```python\n# predictor.py\n\nclass PythonPredictor:\n    def __init__(self, config):\n        self.model = download_model()\n\n    def predict(self, payload):\n        return self.model.predict(payload[\"text\"])\n```\n\n### Configure your deployment\n\n```yaml\n# cortex.yaml\n\n- name: sentiment-classifier\n  predictor:\n    type: python\n    path: predictor.py\n  tracker:\n    model_type: classification\n  compute:\n    gpu: 1\n    mem: 4G\n```\n\n### Deploy to AWS\n\n```bash\n$ cortex deploy\n\ncreating sentiment-classifier\n```\n\n### Serve real-time predictions\n\n```bash\n$ curl http://***.amazonaws.com/sentiment-classifier \\\n    -X POST -H \"Content-Type: application/json\" \\\n    -d '{\"text\": \"the movie was amazing!\"}'\n\npositive\n```\n\n### Monitor your deployment\n\n```bash\n$ cortex get sentiment-classifier --watch\n\nstatus   up-to-date   requested   last update   avg request   2XX\nlive     1            1           8s            24ms          12\n\nclass     count\npositive  8\nnegative  4\n```\n\n<br>\n\n## What is Cortex similar to?\n\nCortex is an open source alternative to serving models with SageMaker or building your own model deployment platform on top of AWS services like Elastic Kubernetes Service (EKS), Elastic Container Service (ECS), Lambda, Fargate, and Elastic Compute Cloud (EC2) and open source projects like Docker, Kubernetes, and TensorFlow Serving.\n\n<br>\n\n## How does Cortex work?\n\nThe CLI sends configuration and code to the cluster every time you run `cortex deploy`. Each model is loaded into a Docker container, along with any Python packages and request handling code. The model is exposed as a web service using Elastic Load Balancing (ELB), TensorFlow Serving, and ONNX Runtime. The containers are orchestrated on Elastic Kubernetes Service (EKS) while logs and metrics are streamed to CloudWatch.\n\n<br>\n\n## Examples of Cortex deployments\n\n<!-- CORTEX_VERSION_README_MINOR x5 -->\n* [Sentiment analysis](https://github.com/cortexlabs/cortex/tree/0.15/examples/tensorflow/sentiment-analyzer): deploy a BERT model for sentiment analysis.\n* [Image classification](https://github.com/cortexlabs/cortex/tree/0.15/examples/tensorflow/image-classifier): deploy an Inception model to classify images.\n* [Search completion](https://github.com/cortexlabs/cortex/tree/0.15/examples/pytorch/search-completer): deploy Facebook's RoBERTa model to complete search terms.\n* [Text generation](https://github.com/cortexlabs/cortex/tree/0.15/examples/pytorch/text-generator): deploy Hugging Face's DistilGPT2 model to generate text.\n* [Iris classification](https://github.com/cortexlabs/cortex/tree/0.15/examples/sklearn/iris-classifier): deploy a scikit-learn model to classify iris flowers.\n"}}}