{"results": {"content_files": {"name": "README.md", "path": "README.md", "content": "<p align=\"center\">\n  <img src=\"doc/source/_static/Alibi_Logo.png\" alt=\"Alibi Logo\" width=\"50%\">\n</p>\n\n[![Build Status](https://travis-ci.com/SeldonIO/alibi.svg?branch=master)](https://travis-ci.com/SeldonIO/alibi)\n[![Documentation Status](https://readthedocs.org/projects/alibi/badge/?version=latest)](https://docs.seldon.io/projects/alibi/en/latest/?badge=latest)\n[![codecov](https://codecov.io/gh/SeldonIO/alibi/branch/master/graph/badge.svg)](https://codecov.io/gh/SeldonIO/alibi)\n![Python version](https://img.shields.io/badge/python-3.6%20%7C%203.7-blue.svg)\n[![PyPI version](https://badge.fury.io/py/alibi.svg)](https://badge.fury.io/py/alibi)\n![GitHub Licence](https://img.shields.io/github/license/seldonio/alibi.svg)\n[![Slack channel](https://img.shields.io/badge/chat-on%20slack-e51670.svg)](http://seldondev.slack.com/messages/alibi)\n---\n[Alibi](https://docs.seldon.io/projects/alibi) is an open source Python library aimed at machine learning model inspection and interpretation.\nThe initial focus on the library is on black-box, instance based model explanations.\n*  [Documentation](https://docs.seldon.io/projects/alibi/en/latest/)\n\nIf you're interested in outlier detection, concept drift or adversarial instance detection, check out our sister project [alibi-detect](https://github.com/SeldonIO/alibi-detect).\n\n## Goals\n* Provide high quality reference implementations of black-box ML model explanation and interpretation algorithms\n* Define a consistent API for interpretable ML methods\n* Support multiple use cases (e.g. tabular, text and image data classification, regression)\n\n## Installation\nAlibi can be installed from [PyPI](https://pypi.org/project/alibi):\n```bash\npip install alibi\n```\nThis will install `alibi` with all its dependencies:\n```bash\n  attrs\n  beautifulsoup4\n  numpy\n  Pillow\n  pandas\n  prettyprinter\n  requests\n  scikit-learn\n  scikit-image\n  scipy\n  shap\n  spacy\n  tensorflow\n```\n\nTo run all the example notebooks, you may additionally run `pip install alibi[examples]` which will\ninstall the following:\n```bash\n  Keras\n  seaborn\n  xgboost\n```\n\n## Supported algorithms\n### Model explanations\nThese algorithms provide **instance-specific** (sometimes also called **local**) explanations of ML model\npredictions. Given a single instance and a model prediction they aim to answer the question \"Why did\nmy model make this prediction?\" The following algorithms all work with **black-box** models meaning that the\nonly requirement is to have acces to a prediction function (which could be an API endpoint for a model in production).\n\nThe following table summarizes the capabilities of the current algorithms:\n\n|Explainer|Model types|Classification|Categorical data|Tabular|Text|Images|Need training set|\n|:---|:---|:---:|:---:|:---:|:---:|:---:|:---|\n|[Anchors](https://docs.seldon.io/projects/alibi/en/latest/methods/Anchors.html)|black-box|\u2714|\u2714|\u2714|\u2714|\u2714|For Tabular|\n|[CEM](https://docs.seldon.io/projects/alibi/en/latest/methods/CEM.html)|black-box, TF/Keras|\u2714|\u2718|\u2714|\u2718|\u2714|Optional|\n|[Counterfactual Instances](https://docs.seldon.io/projects/alibi/en/latest/methods/CF.html)|black-box, TF/Keras|\u2714|\u2718|\u2714|\u2718|\u2714|No|\n|[Kernel SHAP](https://docs.seldon.io/projects/alibi/en/latest/methods/KernelSHAP.html)|black-box|\u2714|\u2714|\u2714|\u2718|\u2718|\u2714|\n|[Prototype Counterfactuals](https://docs.seldon.io/projects/alibi/en/latest/methods/CFProto.html)|black-box, TF/Keras|\u2714|\u2714|\u2714|\u2718|\u2714|Optional|\n\n - Anchor explanations ([Ribeiro et al., 2018](https://homes.cs.washington.edu/~marcotcr/aaai18.pdf))\n   - [Documentation](https://docs.seldon.io/projects/alibi/en/latest/methods/Anchors.html)\n   - Examples:\n     [income prediction](https://docs.seldon.io/projects/alibi/en/latest/examples/anchor_tabular_adult.html),\n     [Iris dataset](https://docs.seldon.io/projects/alibi/en/latest/examples/anchor_tabular_iris.html),\n     [movie sentiment classification](https://docs.seldon.io/projects/alibi/en/latest/examples/anchor_text_movie.html),\n     [ImageNet](https://docs.seldon.io/projects/alibi/en/latest/examples/anchor_image_imagenet.html),\n     [fashion MNIST](https://docs.seldon.io/projects/alibi/en/latest/examples/anchor_image_fashion_mnist.html)\n\n- Contrastive Explanation Method (CEM, [Dhurandhar et al., 2018](https://papers.nips.cc/paper/7340-explanations-based-on-the-missing-towards-contrastive-explanations-with-pertinent-negatives))\n  - [Documentation](https://docs.seldon.io/projects/alibi/en/latest/methods/CEM.html)\n  - Examples: [MNIST](https://docs.seldon.io/projects/alibi/en/latest/examples/cem_mnist.html),\n    [Iris dataset](https://docs.seldon.io/projects/alibi/en/latest/examples/cem_iris.html)\n\n- Counterfactual Explanations (extension of\n  [Wachter et al., 2017](https://arxiv.org/abs/1711.00399))\n  - [Documentation](https://docs.seldon.io/projects/alibi/en/latest/methods/CF.html)\n  - Examples: \n    [MNIST](https://docs.seldon.io/projects/alibi/en/latest/examples/cf_mnist.html)\n\n- Kernel Shapley Additive Explanations ([Lundberg et al., 2017](https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions))\n  - [Documentation](https://docs.seldon.io/projects/alibi/en/latest/methods/KernelSHAP.html)\n  - Examples:\n    [SVM with continuous data](https://docs.seldon.io/projects/alibi/en/latest/examples/kernel_shap_wine_intro.html),\n    [multinomial logistic regression with continous data](https://docs.seldon.io/projects/alibi/en/latest/examples/kernel_shap_wine_lr.html),\n    [handling categorical variables](https://docs.seldon.io/projects/alibi/en/latest/examples/kernel_shap_adult_lr.html)\n    \n- Counterfactual Explanations Guided by Prototypes ([Van Looveren et al., 2019](https://arxiv.org/abs/1907.02584))\n  - [Documentation](https://docs.seldon.io/projects/alibi/en/latest/methods/CFProto.html)\n  - Examples:\n    [MNIST](https://docs.seldon.io/projects/alibi/en/latest/examples/cfproto_mnist.html),\n    [Boston housing dataset](https://docs.seldon.io/projects/alibi/en/latest/examples/cfproto_housing.html),\n    [Adult income (one-hot)](https://docs.seldon.io/projects/alibi/en/latest/examples/cfproto_cat_adult_ohe.html),\n    [Adult income (ordinal)](https://docs.seldon.io/projects/alibi/en/latest/examples/cfproto_cat_adult_ord.html)\n\n### Model confidence metrics\nThese algorihtms provide **instance-specific** scores measuring the model confidence for making a\nparticular prediction.\n\n|Algorithm|Model types|Classification|Regression|Categorical data|Tabular|Text|Images|Need training set|\n|:---|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---|\n|[Trust Scores](https://docs.seldon.io/projects/alibi/en/latest/methods/TrustScores.html)|black-box|\u2714|\u2718|\u2718|\u2714|\u2714(1)|\u2714(2)|Yes|\n|[Linearity Measure](https://docs.seldon.io/projects/alibi/en/latest/examples/linearity_measure_iris.html)|black-box|\u2714|\u2714|\u2718|\u2714|\u2718|\u2714|Optional|\n\n(1) Depending on model\n\n(2) May require dimensionality reduction\n\n- Trust Scores ([Jiang et al., 2018](https://arxiv.org/abs/1805.11783))\n  - [Documentation](https://docs.seldon.io/projects/alibi/en/latest/methods/TrustScores.html)\n  - Examples:\n    [MNIST](https://docs.seldon.io/projects/alibi/en/latest/examples/trustscore_mnist.html),\n    [Iris dataset](https://docs.seldon.io/projects/alibi/en/latest/examples/trustscore_mnist.html)\n- Linearity Measure\n  - Examples:\n    [Iris dataset](https://docs.seldon.io/projects/alibi/en/latest/examples/linearity_measure_iris.html),\n    [fashion MNIST](https://docs.seldon.io/projects/alibi/en/latest/examples/linearity_measure_fashion_mnist.html)\n\n## Example outputs\n\n[**Anchor method applied to the InceptionV3 model trained on ImageNet:**](examples/anchor_image_imagenet.ipynb)\n\nPrediction: Persian Cat             | Anchor explanation\n:-------------------------:|:------------------:\n![Persian Cat](doc/source/methods/persiancat.png)| ![Persian Cat Anchor](doc/source/methods/persiancatanchor.png)\n\n[**Contrastive Explanation method applied to a CNN trained on MNIST:**](examples/cem_mnist.ipynb)\n\nPrediction: 4             |  Pertinent Negative: 9               | Pertinent Positive: 4\n:-------------------------:|:-------------------:|:------------------:\n![mnist_orig](doc/source/methods/mnist_orig.png)  | ![mnsit_pn](doc/source/methods/mnist_pn.png) | ![mnist_pp](doc/source/methods/mnist_pp.png)\n\n[**Trust scores applied to a softmax classifier trained on MNIST:**](examples/trustscore_mnist.ipynb)\n\n![trust_mnist](doc/source/_static/trustscores.png)\n\n## Citations\nIf you use alibi in your research, please consider citing it.\n\nBibTeX entry:\n\n```\n@software{alibi,\n  title = {Alibi: Algorithms for monitoring and explaining machine learning models},\n  author = {Klaise, Janis and Van Looveren, Arnaud and Vacanti, Giovanni and Coca, Alexandru},\n  url = {https://github.com/SeldonIO/alibi},\n  version = {0.4.0},\n  date = {2020-03-20},\n}\n```\n"}}}