{"results": {"content_files": {"name": "README.md", "path": "README.md", "content": "# TorchServe\n\nTorchServe is a flexible and easy to use tool for serving PyTorch models.\n\n**For full documentation, see [Model Server for PyTorch Documentation](docs/README.md).**\n\n## Contents of this Document\n\n* [Install TorchServe](#install-torchserve)\n* [Serve a Model](#serve-a-model)\n* [Quick start with docker](#quick-start-with-docker)\n* [Contributing](#contributing)\n\n## Install TorchServe\n\nConda instructions are provided in more detail, but you may also use `pip` and `virtualenv` if that is your preference.\n**Note:** Java 11 is required. Instructions for installing Java 11 for Ubuntu or macOS are provided in the [Install with Conda](#install-with-conda) section.\n\n### Install with pip\nTo use `pip` to install TorchServe and the model archiver:\n\n```\npip install torch torchtext torchvision sentencepiece\npip install torchserve torch-model-archiver\n```\n\n### Install with Conda\n_Ubuntu_\n\n1. Install Java 11\n    ```bash\n    sudo apt-get install openjdk-11-jdk\n    ```\n1. Install Conda (https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html)\n1. Create an environment and install torchserve and torch-model-archiver\n    For CPU\n    ```bash\n    conda create --name torchserve torchserve torch-model-archiver pytorch torchtext torchvision -c pytorch -c powerai\n    ```\n    For GPU\n    ```bash\n    conda create --name torchserve torchserve torch-model-archiver pytorch torchtext torchvision cudatoolkit=10.1 -c pytorch -c powerai\n    ```\n1. Activate the environment\n    ```bash\n    source activate torchserve\n    ```\n\n_macOS_\n\n1. Install Java 11\n    ```bash\n    brew tap AdoptOpenJDK/openjdk\n    brew cask install adoptopenjdk11\n    ```\n1. Install Conda (https://docs.conda.io/projects/conda/en/latest/user-guide/install/macos.html)\n1. Create an environment and install torchserve and torch-model-archiver\n    ```bash\n    conda create --name torchserve torchserve torch-model-archiver pytorch torchtext torchvision -c pytorch -c powerai\n    ```\n1. Activate the environment\n    ```bash\n    source activate torchserve\n    ```\n\nNow you are ready to [package and serve models with TorchServe](#serve-a-model).\n\n### Install TorchServe for development\n\nIf you plan to develop with TorchServe and change some of the source code, you must install it from source code.\nFirst, clone the repo with:\n\n```bash\ngit clone https://github.com/pytorch/serve\ncd serve\n```\n\nThen make your changes executable with this command:\n\n```bash\npip install -e .\n```\n\n* To develop with torch-model-archiver:\n\n```bash\ncd serve/model-archiver\npip install -e .\n```\n\n* To upgrade TorchServe or model archiver from source code and make changes executable, run:\n\n```bash\npip install -U -e .\n```\n\nFor information about the model archiver, see [detailed documentation](model-archiver/README.md).\n\n## Serve a model\n\nThis section shows a simple example of serving a model with TorchServe. To complete this example, you must have already [installed TorchServe and the model archiver](#install-with-pip).\n\nTo run this example, clone the TorchServe repository and navigate to the root of the repository:\n\n```bash\ngit clone https://github.com/pytorch/serve.git\ncd serve\n```\n\nThen run the following steps from the root of the repository.\n\n### Store a Model\n\nTo serve a model with TorchServe, first archive the model as a MAR file. You can use the model archiver to package a model.\nYou can also create model stores to store your archived models.\n\n1. Create a directory to store your models.\n\n    ```bash\n    mkdir ~/model_store\n    cd ~/model_store\n    ```\n\n1. Download a trained model.\n\n    ```bash\n    wget https://download.pytorch.org/models/densenet161-8d451a50.pth\n    ```\n\n1. Archive the model by using the model archiver. The `extra-files` param uses fa file from the `TorchServe` repo, so update the path if necessary.\n\n    ```bash\n    torch-model-archiver --model-name densenet161 --version 1.0 --model-file ~/serve/examples/image_classifier/densenet_161/model.py --serialized-file ~/model_store/densenet161-8d451a50.pth --extra-files ~/serve/examples/image_classifier/index_to_name.json --handler image_classifier\n    ```\n\nFor more information about the model archiver, see [Torch Model archiver for TorchServe](../model-archiver/README.md)\n\n### Start TorchServe to serve the model\n\nAfter you archive and store the model, use the `torchserve` command to serve the model.\n\n```bash\ntorchserve --start --model-store model_store --models ~/model_store/densenet161=densenet161.mar\n```\n\nAfter you execute the `torchserve` command above, TorchServe runs on your host, listening for inference requests.\n\n**Note**: If you specify model(s) when you run TorchServe, it automatically scales backend workers to the number equal to available vCPUs (if you run on a CPU instance) or to the number of available GPUs (if you run on a GPU instance). In case of powerful hosts with a lot of compute resoures (vCPUs or GPUs). This start up and autoscaling process might take considerable time. If you want to minimize TorchServe start up time you avoid registering and scaling the model during start up time and move that to a later point by using corresponding [Management API](docs/management_api.md#register-a-model), which allows finer grain control of the resources that are allocated for any particular model).\n\n### Get predictions from a model\n\nTo test the model server, send a request to the server's `predictions` API.\n\nComplete the following steps:\n\n* Open a new terminal window (other than the one running TorchServe).\n* Use `curl` to download one of these [cute pictures of a kitten](https://www.google.com/search?q=cute+kitten&tbm=isch&hl=en&cr=&safe=images)\n  and use the  `-o` flag to name it `kitten.jpg` for you.\n* Use `curl` to send `POST` to the TorchServe `predict` endpoint with the kitten's image.\n\n![kitten](docs/images/kitten_small.jpg)\n\nThe following code completes all three steps:\n\n```bash\ncurl -O https://s3.amazonaws.com/model-server/inputs/kitten.jpg\ncurl -X POST http://127.0.0.1:8080/predictions/densenet161 -T kitten.jpg\n```\n\nThe predict endpoint returns a prediction response in JSON. It will look something like the following result:\n\n```json\n[\n  {\n    \"tiger_cat\": 0.46933549642562866\n  },\n  {\n    \"tabby\": 0.4633878469467163\n  },\n  {\n    \"Egyptian_cat\": 0.06456148624420166\n  },\n  {\n    \"lynx\": 0.0012828214094042778\n  },\n  {\n    \"plastic_bag\": 0.00023323034110944718\n  }\n]\n```\n\nYou will see this result in the response to your `curl` call to the predict endpoint, and in the server logs in the terminal window running TorchServe. It's also being [logged locally with metrics](docs/metrics.md).\n\nNow you've seen how easy it can be to serve a deep learning model with TorchServe! [Would you like to know more?](docs/server.md)\n\n### Stop the running TorchServe\n\nTo stop the currently running TorchServe instance, run the following command:\n\n```bash\ntorchserve --stop\n```\n\nYou see output specifying that TorchServe has stopped.\n\n## Quick Start with Docker\n\n### Prerequisites\n\n* docker - Refer to the [official docker installation guide](https://docs.docker.com/install/)\n* git    - Refer to the [official git set-up guide](https://help.github.com/en/github/getting-started-with-github/set-up-git)\n* TorchServe source code. Clone and enter the repo as follows:\n\n```bash\ngit clone https://github.com/pytorch/serve.git\ncd serve\n```\n\n### Build the TorchServe Docker image\n\nThe following are examples on how to use the `build_image.sh` script to build Docker images to support CPU or GPU inference.\n\nTo build the TorchServe image for a CPU device using the `master` branch, use the following command:\n\n```bash\n./build_image.sh\n```\n\nTo create a Docker image for a specific branch, use the following command:\n\n```bash\n./build_image.sh -b <branch_name>\n```\n\nTo create a Docker image for a GPU device, use the following command:\n\n```bash\n./build_image.sh --gpu\n```\n\nTo create a Docker image for a GPU device with a specific branch, use following command:\n\n```bash\n./build_image.sh -b <branch_name> --gpu\n```\n\nTo run your TorchServe Docker image and start TorchServe inside the container with a pre-registered `resnet-18` image classification model, use the following command:\n\n```bash\n./start.sh\n```\n\n## Learn More\n\n* [Full documentation on TorchServe](docs/README.md)\n* [Manage models API](docs/management_api.md)\n* [Inference API](docs/inference_api.md)\n* [Package models for use with TorchServe](model-archiver/README.md)\n\n## Contributing\n\nWe welcome all contributions!\n\nTo learn more about how to contribute, see the contributor guide [here](https://github.com/pytorch/serve/blob/master/CONTRIBUTING.md). \n\nTo file a bug or request a feature, please file a GitHub issue. For filing pull requests, please use the template [here](https://github.com/pytorch/serve/blob/master/pull_request_template.md). Cheers!\n"}}}