{"results": {"content_files": {"name": "README.md", "path": "README.md", "content": "[![License](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](LICENSE)[![CircleCI](https://circleci.com/gh/pytorch/elastic.svg?style=svg&circle-token=9bea46e94adbe2f3e0fb2d4054b1b655f2e208c2)](https://circleci.com/gh/pytorch/elastic)\n\n# TorchElastic\n\nTorchElastic allows you to launch distributed PyTorch jobs in a\nfault-tolerant and elastic manner.\nFor the latest documentation, please refer to our\n[website](https://pytorch.org/elastic).\n\n\n## Requirements\ntorchelastic requires\n* python3 (3.6+)\n* torch\n* etcd\n\n## Installation\n```bash\npip install torchelastic\n```\n\n## Quickstart\n\n**Fault-tolerant** on `4` nodes, `8` trainers/node, total `4 * 8 = 32` trainers.\nRun the following on all nodes.\n```bash\npython -m torchelastic.distributed.launch\n            --nnodes=4\n            --nproc_per_node=8\n            --rdzv_id=JOB_ID\n            --rdzv_backend=etcd\n            --rdzv_endpoint=ETCD_HOST:ETCD_PORT\n            YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)\n```\n\n**Elastic on** `1 ~ 4` nodes, `8` trainers/node, total `8 ~ 32` trainers. Job\nstarts as soon as `1` node is healthy, you may add up to `4` nodes.\n```bash\npython -m torchelastic.distributed.launch\n            --nnodes=1:4\n            --nproc_per_node=8\n            --rdzv_id=JOB_ID\n            --rdzv_backend=etcd\n            --rdzv_endpoint=ETCD_HOST:ETCD_PORT\n            YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)\n\n```\n## Contributing\n\nWe welcome PRs. See the [CONTRIBUTING](CONTRIBUTING.md) file.\n\n## License\ntorchelastic is BSD licensed, as found in the [LICENSE](LICENSE) file.\n"}}}