{"results": {"content_files": {"name": "README.md", "path": "README.md", "content": "# Deep Graph Library (DGL)\n\n[![PyPi Latest Release](https://img.shields.io/pypi/v/dgl.svg)](https://pypi.org/project/dgl/)\n[![Conda Latest Release](https://anaconda.org/dglteam/dgl/badges/version.svg)](https://anaconda.org/dglteam/dgl)\n[![Build Status](http://ci.dgl.ai:80/buildStatus/icon?job=DGL/master)](http://ci.dgl.ai:80/job/DGL/job/master/)\n[![Benchmark by ASV](http://img.shields.io/badge/benchmarked%20by-asv-green.svg?style=flat)](https://asv.dgl.ai/)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](./LICENSE)\n\nDocumentation ([Latest](https://docs.dgl.ai/en/latest/) | [Stable](https://docs.dgl.ai)) | [DGL at a glance](https://docs.dgl.ai/tutorials/basics/1_first.html#sphx-glr-tutorials-basics-1-first-py) | [Model Tutorials](https://docs.dgl.ai/tutorials/models/index.html) | [Discussion Forum](https://discuss.dgl.ai)\n\n\nDGL is an easy-to-use, high performance and scalable Python package for deep learning on graphs. DGL is framework agnostic, meaning if a deep graph model is a component of an end-to-end application, the rest of the logics can be implemented in any major frameworks, such as PyTorch, Apache MXNet or TensorFlow.\n\n<p align=\"center\">\n  <img src=\"http://data.dgl.ai/asset/image/DGL-Arch.png\" alt=\"DGL v0.4 architecture\" width=\"600\">\n  <br>\n  <b>Figure</b>: DGL Overall Architecture\n</p>\n\n## <img src=\"http://data.dgl.ai/asset/image/new.png\" width=\"30\">DGL News\n*03/31/2020*: The new **v0.4.3 release** includes official TensorFlow support, with 15 popular GNN modules. DGL-KE and DGL-LifeSci, two packages for knowledge graph embedding and chemi- and bio-informatics respectively, have graduated as standalone packages and can be installed by pip and conda. The new release provides full support of graph sampling on heterogeneous graphs, with multi-GPU acceleration. See our [new feature walkthrough](https://www.dgl.ai/release/2020/04/01/release.html) and [release note](https://github.com/dmlc/dgl/releases/tag/0.4.3).\n\n*03/02/2020*: **Check out this cool paper: [Benchmarking Graph Neural Networks](https://arxiv.org/abs/2003.00982)!**  It includes a DGL-based benchmark framework for novel medium-scale graph datasets, covering mathematical modeling, computer vision, chemistry and combinatorial problems.  See [repo here](https://github.com/graphdeeplearning/benchmarking-gnns).\n\n## Using DGL\n\n**A data scientist** may want to apply a pre-trained model to your data right away. For this you can use DGL's [Application packages, formally *Model Zoo*](https://github.com/dmlc/dgl/tree/master/apps). Application packages are developed for domain applications, as is the case for [DGL-LifeScience](https://github.com/dmlc/dgl/tree/master/apps/life_sci). We will soon add model zoo for knowledge graph embedding learning and recommender systems. Here's how you will use a pretrained model:\n```python\nfrom dgllife.data import Tox21\nfrom dgllife.model import load_pretrained\nfrom dgllife.utils import smiles_to_bigraph, CanonicalAtomFeaturizer\n\ndataset = Tox21(smiles_to_bigraph, CanonicalAtomFeaturizer())\nmodel = load_pretrained('GCN_Tox21') # Pretrained model loaded\nmodel.eval()\n\nsmiles, g, label, mask = dataset[0]\nfeats = g.ndata.pop('h')\nlabel_pred = model(g, feats)\nprint(smiles)                   # CCOc1ccc2nc(S(N)(=O)=O)sc2c1\nprint(label_pred[:, mask != 0]) # Mask non-existing labels\n# tensor([[ 1.4190, -0.1820,  1.2974,  1.4416,  0.6914,  \n# 2.0957,  0.5919,  0.7715, 1.7273,  0.2070]])\n```\n\n**Further reading**: DGL is released as a managed service on AWS SageMaker, see the medium posts for an easy trip to DGL on SageMaker([part1](https://medium.com/@julsimon/a-primer-on-graph-neural-networks-with-amazon-neptune-and-the-deep-graph-library-5ce64984a276) and [part2](https://medium.com/@julsimon/deep-graph-library-part-2-training-on-amazon-sagemaker-54d318dfc814)).\n\n**Researchers** can start from the growing list of [models implemented in DGL](https://github.com/dmlc/dgl/tree/master/examples). Developing new models does not mean that you have to start from scratch. Instead, you can reuse many [pre-built modules](https://docs.dgl.ai/api/python/nn.html). Here is how to get a standard two-layer graph convolutional model with a pre-built GraphConv module:\n```python\nfrom dgl.nn.pytorch import GraphConv\nimport torch.nn.functional as F\n\n# build a two-layer GCN with ReLU as the activation in between\nclass GCN(nn.Module):\n    def __init__(self, in_feats, h_feats, num_classes):\n        super(GCN, self).__init__()\n        self.gcn_layer1 = GraphConv(in_feats, h_feats)\n        self.gcn_layer2 = GraphConv(h_feats, num_classes)\n    \n    def forward(self, graph, inputs):\n        h = self.gcn_layer1(graph, inputs)\n        h = F.relu(h)\n        h = self.gcn_layer2(graph, h)\n        return h\n```\n\nNext level down, you may want to innovate your own module. DGL offers a succinct message-passing interface (see tutorial [here](https://docs.dgl.ai/tutorials/basics/3_pagerank.html)). Here is how Graph Attention Network (GAT) is implemented ([complete codes](https://docs.dgl.ai/api/python/nn.pytorch.html#gatconv)). Of course, you can also find GAT as a module [GATConv](https://docs.dgl.ai/api/python/nn.pytorch.html#gatconv):\n```python\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Define a GAT layer\nclass GATLayer(nn.Module):\n    def __init__(self, in_feats, out_feats):\n        super(GATLayer, self).__init__()\n        self.linear_func = nn.Linear(in_feats, out_feats, bias=False)\n        self.attention_func = nn.Linear(2 * out_feats, 1, bias=False)\n        \n    def edge_attention(self, edges):\n        concat_z = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n        src_e = self.attention_func(concat_z)\n        src_e = F.leaky_relu(src_e)\n        return {'e': src_e}\n    \n    def message_func(self, edges):\n        return {'z': edges.src['z'], 'e':edges.data['e']}\n        \n    def reduce_func(self, nodes):\n        a = F.softmax(nodes.mailbox['e'], dim=1)\n        h = torch.sum(a * nodes.mailbox['z'], dim=1)\n        return {'h': h}\n                               \n    def forward(self, graph, h):\n        z = self.linear_func(h)\n        graph.ndata['z'] = z\n        graph.apply_edges(self.edge_attention)\n        graph.update_all(self.message_func, self.reduce_func)\n        return graph.ndata.pop('h')\n```\n## Performance and Scalability\n\n**Microbenchmark on speed and memory usage**: While leaving tensor and autograd functions to backend frameworks (e.g. PyTorch, MXNet, and TensorFlow), DGL aggressively optimizes storage and computation with its own kernels. Here's a comparison to another popular package -- PyTorch Geometric (PyG). The short story is that raw speed is similar, but DGL has much better memory management.\n\n\n| Dataset  |    Model     |                   Accuracy                   |                    Time <br> PyG &emsp;&emsp; DGL                    |           Memory <br> PyG &emsp;&emsp; DGL            |\n| -------- |:------------:|:--------------------------------------------:|:--------------------------------------------------------------------:|:-----------------------------------------------------:|\n| Cora     | GCN <br> GAT | 81.31 &plusmn; 0.88 <br> 83.98 &plusmn; 0.52 | <b>0.478</b> &emsp;&emsp; 0.666 <br> 1.608 &emsp;&emsp; <b>1.399</b> | 1.1 &emsp;&emsp; 1.1 <br> 1.2 &emsp;&emsp; <b>1.1</b> |\n| CiteSeer | GCN <br> GAT | 70.98 &plusmn; 0.68 <br> 69.96 &plusmn; 0.53 | <b>0.490</b> &emsp;&emsp; 0.674 <br> 1.606 &emsp;&emsp; <b>1.399</b> | 1.1 &emsp;&emsp; 1.1 <br> 1.3 &emsp;&emsp; <b>1.1</b> |\n| PubMed   | GCN <br> GAT | 79.00 &plusmn; 0.41 <br> 77.65 &plusmn; 0.32 | <b>0.491</b> &emsp;&emsp; 0.690 <br> 1.946 &emsp;&emsp; <b>1.393</b> | 1.1 &emsp;&emsp; 1.1 <br> 1.6 &emsp;&emsp; <b>1.1</b> |\n| Reddit   |     GCN      |             93.46 &plusmn; 0.06              |                    *OOM*&emsp;&emsp; <b>28.6</b>                     |            *OOM* &emsp;&emsp; <b>11.7</b>             |\n| Reddit-S |     GCN      |                     N/A                      |                    29.12 &emsp;&emsp; <b>9.44</b>                    |             15.7 &emsp;&emsp; <b>3.6</b>              |\n\nTable: Training time(in seconds) for 200 epochs and memory consumption(GB)\n\nHere is another comparison of DGL on TensorFlow backend with other TF-based GNN tools (training time in seconds for one epoch):\n\n| Dateset | Model | DGL | GraphNet | tf_geometric |\n| ------- | ----- | --- | -------- | ------------ |\n| Core | GCN | 0.0148 | 0.0152 | 0.0192 |\n| Reddit | GCN | 0.1095 | OOM | OOM |\n| PubMed | GCN | 0.0156 | 0.0553 | 0.0185 |\n| PPI | GCN | 0.09 | 0.16 | 0.21 |\n| Cora | GAT | 0.0442 | n/a | 0.058 |\n| PPI | GAT | 0.398 | n/a | 0.752 |\n\nHigh memory utilization allows DGL to push the limit of single-GPU performance, as seen in below images.\n| <img src=\"http://data.dgl.ai/asset/image/DGLvsPyG-time1.png\" width=\"400\"> | <img src=\"http://data.dgl.ai/asset/image/DGLvsPyG-time2.png\" width=\"400\"> |\n| -------- | -------- |\n\n**Scalability**: DGL has fully leveraged multiple GPUs in both one machine and clusters for increasing training speed, and has better performance than alternatives, as seen in below images.\n\n<p align=\"center\">\n  <img src=\"http://data.dgl.ai/asset/image/one-four-GPUs.png\" width=\"600\">\n</p>\n\n| <img src=\"http://data.dgl.ai/asset/image/one-four-GPUs-DGLvsGraphVite.png\"> |  <img src=\"http://data.dgl.ai/asset/image/one-fourMachines.png\"> | \n| :---------------------------------------: | -- |\n\n\n**Further reading**: Detailed comparison of DGL and other Graph alternatives can be found [here](https://arxiv.org/abs/1909.01315).\n\n## DGL Models and Applications\n\n### DGL for research\nOverall there are 30+ models implemented by using DGL:\n- [PyTorch](https://github.com/dmlc/dgl/tree/master/examples/pytorch)\n- [MXNet](https://github.com/dmlc/dgl/tree/master/examples/mxnet)\n- [TensorFlow](https://github.com/dmlc/dgl/tree/master/examples/tensorflow)\n\n### DGL for domain applications\n- [DGL-LifeSci](https://github.com/dmlc/dgl/tree/master/apps/life_sci), previously DGL-Chem\n- [DGL-KE](https://github.com/awslabs/dgl-ke)\n- DGL-RecSys(coming soon)\n\n### DGL for NLP/CV problems\n- [TreeLSTM](https://github.com/dmlc/dgl/tree/master/examples/pytorch/tree_lstm)\n- [GraphWriter](https://github.com/dmlc/dgl/tree/master/examples/pytorch/graphwriter)\n- [Capsule Network](https://github.com/dmlc/dgl/tree/master/examples/pytorch/capsule)\n\nWe are currently in Beta stage.  More features and improvements are coming.\n\n\n## Installation\n\nDGL should work on\n\n* all Linux distributions no earlier than Ubuntu 16.04\n* macOS X\n* Windows 10\n\nDGL requires Python 3.5 or later.\n\nRight now, DGL works on [PyTorch](https://pytorch.org) 1.2.0+, [MXNet](https://mxnet.apache.org) 1.5.1+, and [TensorFlow](https://tensorflow.org) 2.1.0+.\n\n\n### Using anaconda\n\n```\nconda install -c dglteam dgl           # cpu version\nconda install -c dglteam dgl-cuda9.0   # CUDA 9.0\nconda install -c dglteam dgl-cuda9.2   # CUDA 9.2\nconda install -c dglteam dgl-cuda10.0  # CUDA 10.0\nconda install -c dglteam dgl-cuda10.1  # CUDA 10.1\n```\n\n### Using pip\n\n\n|           | Latest Nightly Build Version  | Stable Version          |\n|-----------|-------------------------------|-------------------------|\n| CPU       | `pip install --pre dgl`       | `pip install dgl`       |\n| CUDA 9.0  | `pip install --pre dgl-cu90`  | `pip install dgl-cu90`  |\n| CUDA 9.2  | `pip install --pre dgl-cu92`  | `pip install dgl-cu92`  |\n| CUDA 10.0 | `pip install --pre dgl-cu100` | `pip install dgl-cu100` |\n| CUDA 10.1 | `pip install --pre dgl-cu101` | `pip install dgl-cu101` |\n\n### Built from source code\n\nRefer to the guide [here](https://docs.dgl.ai/install/index.html#install-from-source).\n\n\n## DGL Major Releases\n\n| Releases  | Date   | Features |\n|-----------|--------|-------------------------|\n| v0.4.3    | 03/31/2020 | - TensorFlow support <br> - DGL-KE <br> - DGL-LifeSci <br> - Heterograph sampling APIs (experimental) |\n| v0.4.2      | 01/24/2020 |  - Heterograph support <br> - TensorFlow support (experimental) <br> - MXNet GNN modules <br> | \n| v0.3.1 | 08/23/2019 | - APIs for GNN modules <br> - Model zoo (DGL-Chem) <br> - New installation |\n| v0.2 | 03/09/2019 | - Graph sampling APIs <br> - Speed improvement |\n| v0.1 | 12/07/2018 | - Basic DGL APIs <br> - PyTorch and MXNet support <br> - GNN model examples and tutorials |\n\n## New to Deep Learning and Graph Deep Learning?\n\nCheck out the open source book [*Dive into Deep Learning*](http://gluon.ai/).\n\nFor those who are new to graph neural network, please see the [basic of DGL](https://docs.dgl.ai/tutorials/basics/index.html).\n\nFor audience who are looking for more advanced, realistic, and end-to-end examples, please see [model tutorials](https://docs.dgl.ai/tutorials/models/index.html).\n\n\n## Contributing\n\nPlease let us know if you encounter a bug or have any suggestions by [filing an issue](https://github.com/dmlc/dgl/issues).\n\nWe welcome all contributions from bug fixes to new features and extensions.\n\nWe expect all contributions discussed in the issue tracker and going through PRs.  Please refer to our [contribution guide](https://docs.dgl.ai/contribute.html).\n\n## Cite\n\nIf you use DGL in a scientific publication, we would appreciate citations to the following paper:\n```\n@article{wang2019dgl,\n    title={Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs},\n    url={https://arxiv.org/abs/1909.01315},\n    author={Wang, Minjie and Yu, Lingfan and Zheng, Da and Gan, Quan and Gai, Yu and Ye, Zihao and Li, Mufei and Zhou, Jinjing and Huang, Qi and Ma, Chao and Huang, Ziyue and Guo, Qipeng and Zhang, Hao and Lin, Haibin and Zhao, Junbo and Li, Jinyang and Smola, Alexander J and Zhang, Zheng},\n    journal={ICLR Workshop on Representation Learning on Graphs and Manifolds},\n    year={2019}\n}\n```\n\n## The Team\n\nDGL is developed and maintained by [NYU, NYU Shanghai, AWS Shanghai AI Lab, and AWS MXNet Science Team](https://www.dgl.ai/pages/about.html).\n\n\n## License\n\nDGL uses Apache License 2.0.\n"}}}