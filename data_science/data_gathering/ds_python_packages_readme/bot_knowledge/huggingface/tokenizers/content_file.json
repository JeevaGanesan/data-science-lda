{"results": {"content_files": {"name": "README.md", "path": "README.md", "content": "<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png\" width=\"600\"/>\n    <br>\n<p>\n<p align=\"center\">\n    <img alt=\"Build\" src=\"https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg\">\n    <a href=\"https://github.com/huggingface/tokenizers/blob/master/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue&cachedrop\">\n    </a>\n    </a>\n    <a href=\"https://pepy.tech/project/tokenizers/week\">\n        <img src=\"https://pepy.tech/badge/tokenizers/week\" />\n    </a>\n</p>\n\nProvides an implementation of today's most used tokenizers, with a focus on performance and\nversatility.\n\n## Main features:\n\n - Train new vocabularies and tokenize, using today's most used tokenizers.\n - Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes\n   less than 20 seconds to tokenize a GB of text on a server's CPU.\n - Easy to use, but also extremely versatile.\n - Designed for research and production.\n - Normalization comes with alignments tracking. It's always possible to get the part of the\n   original sentence that corresponds to a given token.\n - Does all the pre-processing: Truncate, Pad, add the special tokens your model needs.\n\n## Bindings\n\nWe provide bindings to the following languages (more to come!):\n  - [Rust](https://github.com/huggingface/tokenizers/tree/master/tokenizers) (Original implementation)\n  - [Python](https://github.com/huggingface/tokenizers/tree/master/bindings/python)\n  - [Node.js](https://github.com/huggingface/tokenizers/tree/master/bindings/node)\n \n## Quick examples using Python:\n\nStart using in a matter of seconds:\n\n```python\n# Tokenizers provides ultra-fast implementations of most current tokenizers:\n>>> from tokenizers import (ByteLevelBPETokenizer,\n                            CharBPETokenizer,\n                            SentencePieceBPETokenizer,\n                            BertWordPieceTokenizer)\n# Ultra-fast => they can encode 1GB of text in ~20sec on a standard server's CPU\n# Tokenizers can be easily instantiated from standard files\n>>> tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\nTokenizer(vocabulary_size=30522, model=BertWordPiece, add_special_tokens=True, unk_token=[UNK], \n          sep_token=[SEP], cls_token=[CLS], clean_text=True, handle_chinese_chars=True, \n          strip_accents=True, lowercase=True, wordpieces_prefix=##)\n\n# Tokenizers provide exhaustive outputs: tokens, mapping to original string, attention/special token masks.\n# They also handle model's max input lengths as well as padding (to directly encode in padded batches)\n>>> output = tokenizer.encode(\"Hello, y'all! How are you \ud83d\ude01 ?\")\nEncoding(num_tokens=13, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing, original_str, normalized_str])\n>>> print(output.ids, output.tokens, output.offsets)\n[101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 100, 1029, 102]\n['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are', 'you', '[UNK]', '?', '[SEP]']\n[(0, 0), (0, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13), (14, 17), (18, 21), (22, 25), (26, 27),\n (28, 29), (0, 0)]\n# Here is an example using the offsets mapping to retrieve the string corresponding to the 10th token:\n>>> output.original_str[output.offsets[10]]\n'\ud83d\ude01'\n```\n\nAnd training a new vocabulary is just as easy:\n\n```python\n# You can also train a BPE/Byte-levelBPE/WordPiece vocabulary on your own files\n>>> tokenizer = ByteLevelBPETokenizer()\n>>> tokenizer.train([\"wiki.test.raw\"], vocab_size=20000)\n[00:00:00] Tokenize words                 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   20993/20993\n[00:00:00] Count pairs                    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   20993/20993\n[00:00:03] Compute merges                 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   19375/19375\n```\n \n## Contributors\n  \n[![](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/images/0)](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/links/0)[![](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/images/1)](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/links/1)[![](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/images/2)](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/links/2)[![](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/images/3)](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/links/3)[![](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/images/4)](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/links/4)[![](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/images/5)](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/links/5)[![](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/images/6)](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/links/6)[![](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/images/7)](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/links/7)\n\n"}}}